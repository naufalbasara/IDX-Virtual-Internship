# -*- coding: utf-8 -*-
"""2 Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cigooNzf4bT0d0bAE6hoKoZ7Z4OQ-N_Z
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor

base_path = '/content/drive/MyDrive/Internship/IDX Partners'
data_path = os.path.join(base_path, 'Dataset')

df = pd.read_csv(os.path.join(data_path, 'cleaned-model-v1.csv'))
df.drop(['Unnamed: 0'], axis=1, inplace=True)
df.info()

"""## Feature Engineering"""

# Feature Standardization/Normalization

# Standardize the feature
from sklearn.preprocessing import StandardScaler

sc = StandardScaler() # creating an instance of the class object (x-mu/sd)
df[['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'annual_inc', 'dti', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'last_pymnt_amnt', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']] = sc.fit_transform(df[['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'annual_inc', 'dti', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'last_pymnt_amnt', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']])

df.head()

"""### Split the dataset"""

# Split training and test set
from sklearn.model_selection import train_test_split

X = df.iloc[:, :-1]
y = df.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""#### Oversampling"""

# example of random oversampling to balance the class distribution
from collections import Counter
from imblearn.over_sampling import RandomOverSampler

# summarize class distribution
print(Counter(y))
# define oversampling strategy
oversample = RandomOverSampler(sampling_strategy='minority')
# fit and apply the transform
X_over, y_over = oversample.fit_resample(X, y)
# summarize class distribution
print(Counter(y_over))

"""#### Feature Importance"""

# define the model
model = RandomForestRegressor()
# fit the model
model.fit(X_train, y_train)
# get importance
importance = model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
 print(f'Feature: {df.columns[i]}, Score: {v}')
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

"""## Modelling

### Training
"""

model_metrics = {
    'logistic_regression': {
        'accuracy': 0,
        'precision': 0,
        'recall': 0,
        'f1': 0,
    },
    'random_forest': {
        'accuracy': 0,
        'precision': 0,
        'recall': 0,
        'f1': 0,
    },
    'decision_tree': {
        'accuracy': 0,
        'precision': 0,
        'recall': 0,
        'f1': 0,
    }
}

"""##### Logistic Regression Model

###### Before oversampling
"""

# Training before oversampling
from sklearn.linear_model import LogisticRegression

model_log = LogisticRegression()
model_log.fit(X_train, y_train)

y_pred = model_log.predict(X_test)

# Model evaluation before oversampling
from sklearn import metrics

cm = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(cm,
            annot=True,
            fmt='g',
            xticklabels=['Risky','Good'],
            yticklabels=['Risky','Good'])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Finding precision and recall
accuracy = accuracy_score(y_test, y_pred)
model_metrics['logistic_regression']['accuracy'] = accuracy
print("Logistic Regression Accuracy   :", accuracy)

precision = precision_score(y_test, y_pred)
model_metrics['logistic_regression']['precision'] = precision
print("Logistic Regression Precision :", precision)

recall = recall_score(y_test, y_pred)
model_metrics['logistic_regression']['recall'] = recall
print("Logistic Regression Recall    :", recall)

F1_score = f1_score(y_test, y_pred)
model_metrics['logistic_regression']['f1'] = F1_score
print("Logistic Regression F1-score  :", F1_score)

"""###### After oversampling"""

# Training after oversampling
from sklearn.linear_model import LogisticRegression

model_log = LogisticRegression()
model_log.fit(X_train, y_train)

y_pred = model_log.predict(X_test)

# Model evaluation after oversampling
from sklearn import metrics

cm = metrics.confusion_matrix(y_test, y_pred)
sns.heatmap(cm,
            annot=True,
            fmt='g',
            xticklabels=['Risky','Good'],
            yticklabels=['Risky','Good'])

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Finding precision and recall after oversampling
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy   :", accuracy)
precision = precision_score(y_test, y_pred)
print("Precision :", precision)
recall = recall_score(y_test, y_pred)
print("Recall    :", recall)
F1_score = f1_score(y_test, y_pred)
print("F1-score  :", F1_score)

"""##### Random Forest Model"""

# Training before oversampling
from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

y_pred_rf = model_rf.predict(X_test)

# Model evaluation before oversampling
from sklearn import metrics

cm = metrics.confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm,
            annot=True,
            fmt='g',
            xticklabels=['Risky','Good'],
            yticklabels=['Risky','Good'])

plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Finding precision and recall
accuracy = accuracy_score(y_test, y_pred_rf)
model_metrics['random_forest']['accuracy'] = accuracy
print("Random Forest Accuracy   :", accuracy)

precision = precision_score(y_test, y_pred_rf)
model_metrics['random_forest']['precision'] = precision
print("Random Forest Precision :", precision)

recall = recall_score(y_test, y_pred_rf)
model_metrics['random_forest']['recall'] = recall
print("Random Forest Recall    :", recall)

F1_score = f1_score(y_test, y_pred_rf)
model_metrics['random_forest']['f1'] = F1_score
print("Random Forest F1-score  :", F1_score)

"""##### Decision Tree Model"""

from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier

# Create Decision Tree classifer object
model_dt = DecisionTreeClassifier()

# Train Decision Tree Classifer
model_dt = model_dt.fit(X_train,y_train)

#Predict the response for test dataset
y_pred_dt = model_dt.predict(X_test)

# Model evaluation before oversampling
from sklearn import metrics

cm = metrics.confusion_matrix(y_test, y_pred_dt)
sns.heatmap(cm,
            annot=True,
            fmt='g',
            xticklabels=['Risky','Good'],
            yticklabels=['Risky','Good'])

plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix',fontsize=17)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Finding precision and recall
accuracy = accuracy_score(y_test, y_pred_dt)
model_metrics['decision_tree']['accuracy'] = accuracy
print("Decision Tree Accuracy   :", accuracy)

precision = precision_score(y_test, y_pred_dt)
model_metrics['decision_tree']['precision'] = precision
print("Decision Tree Precision :", precision)

recall = recall_score(y_test, y_pred_dt)
model_metrics['decision_tree']['recall'] = recall
print("Decision Tree Recall    :", recall)

F1_score = f1_score(y_test, y_pred_dt)
model_metrics['decision_tree']['f1'] = F1_score
print("Decision Tree F1-score  :", F1_score)

model_metrics

print(f'model with the highest accuracy: {sorted(model_metrics, key=lambda x: model_metrics[x]["accuracy"], reverse=True)[0]}')
for x in model_metrics.keys():
  print(f'{x}: {model_metrics[x]["accuracy"]}')

print(f'model with the highest precision: {sorted(model_metrics, key=lambda x: model_metrics[x]["precision"], reverse=True)[0]}')
for x in model_metrics.keys():
  print(f'{x}: {model_metrics[x]["precision"]}')

print(f'model with the highest recall: {sorted(model_metrics, key=lambda x: model_metrics[x]["recall"], reverse=True)[0]}')
for x in model_metrics.keys():
  print(f'{x}: {model_metrics[x]["recall"]}')

print(f'model with the highest f1 score: {sorted(model_metrics, key=lambda x: model_metrics[x]["f1"], reverse=True)[0]}')
for x in model_metrics.keys():
  print(f'{x}: {model_metrics[x]["f1"]}')

"""## Assess Risk of Credit Loan"""

os.mkdir(os.path.join(base_path,'trained_model'))

import pickle

# save the model to disk
filename = os.path.join(base_path, 'trained_model', 'rf_model.sav')
pickle.dump(model_rf, open(filename, 'wb'))

# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test, y_test)
print(result)

